{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following file displays the results for the model: h2o-danube3-500m-chat (500M parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import Dataset\n",
    "from accelerate import Accelerator, init_empty_weights, infer_auto_device_map, load_checkpoint_and_dispatch\n",
    "import torch\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Initialize the device\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "WANDB_API_KEY = os.getenv('WANDB_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msejnimka\u001b[0m (\u001b[33msejnimka-indiana-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\sejal\\_netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logging in to Weights and Biases account\n",
    "wandb.login(key=WANDB_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\AML\\wandb\\run-20241215_152911-klfgn9fo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sejnimka-indiana-university/AML_HW4_Q2/runs/klfgn9fo' target=\"_blank\">h20ai-data-tuning</a></strong> to <a href='https://wandb.ai/sejnimka-indiana-university/AML_HW4_Q2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sejnimka-indiana-university/AML_HW4_Q2' target=\"_blank\">https://wandb.ai/sejnimka-indiana-university/AML_HW4_Q2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sejnimka-indiana-university/AML_HW4_Q2/runs/klfgn9fo' target=\"_blank\">https://wandb.ai/sejnimka-indiana-university/AML_HW4_Q2/runs/klfgn9fo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/sejnimka-indiana-university/AML_HW4_Q2/runs/klfgn9fo?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2694e73c2e0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initializing wandb project\n",
    "wandb.init(project=\"AML_HW4_Q2\", \n",
    "           name=\"h20ai-data-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_HOME\"] = \"./huggingface_cache\"\n",
    "os.environ['HF_TOKEN'] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick an existing fine-tuning dataset (or create your own - you can create it by scraping the web or using some of the documents you have on your computer (do not use anything confidential or copyrighted) or by asking ChatGPT to generate the data). Mention which dataset you picked/created and briefly describe the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    {\n",
    "        \"input\": \"I failed my physics exam yesterday. I was so disappointed with myself.\",\n",
    "        \"response\": \"I'm sorry to hear that. Don't be too hard on yourself. Maybe you can focus on the topics that troubled you the most for the next test.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"I got promoted at work today!\",\n",
    "        \"response\": \"Congratulations! That's amazing news. You must feel so proud of yourself!\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"The weather is so gloomy today.\",\n",
    "        \"response\": \"It is. Maybe you can cozy up with a book or watch a nice movie to brighten your mood.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"I'm thinking of starting a new hobby, but I don't know what to choose.\",\n",
    "        \"response\": \"That's a great idea! What about something creative like painting or practical like gardening?\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"I argued with my best friend, and now I feel terrible.\",\n",
    "        \"response\": \"I'm sorry to hear that. Maybe give them some time and then try to talk it out calmly. True friendships are worth it.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"I'm so nervous about my interview tomorrow.\",\n",
    "        \"response\": \"It's normal to feel nervous, but remember to prepare well and believe in yourself. You've got this!\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What do you think about the new superhero movie?\",\n",
    "        \"response\": \"I loved it! The action scenes were incredible, and the storyline was so engaging.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"I think I'm addicted to my phone. I spend way too much time on it.\",\n",
    "        \"response\": \"You're not alone! Maybe try setting limits or scheduling phone-free activities to help you cut down gradually.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Do you think I should start exercising?\",\n",
    "        \"response\": \"Definitely! Even small steps like a daily walk can make a big difference to your health and mood.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"I can't decide whether to buy this new laptop or wait for a sale.\",\n",
    "        \"response\": \"If you can wait and it's not urgent, a sale might save you some money. But if you need it now, go for it!\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"I feel so overwhelmed with work lately.\",\n",
    "        \"response\": \"I'm sorry you're feeling this way. Maybe taking short breaks and prioritizing tasks can help ease the load.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"I just broke up with my partner, and I feel lost.\",\n",
    "        \"response\": \"I'm really sorry to hear that. It's okay to feel this way. Take time to heal and lean on your friends and family for support.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"I got rejected for the job I really wanted.\",\n",
    "        \"response\": \"I'm sorry to hear that. Rejections are tough, but every step brings you closer to the right opportunity. Don't give up!\"\n",
    "    },\n",
    "    # Daily Life and Practical Advice\n",
    "    {\n",
    "        \"input\": \"I want to cook something special tonight. Any suggestions?\",\n",
    "        \"response\": \"How about trying a homemade pasta dish or a classic roast chicken? Both are crowd-pleasers!\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"My plants keep dying no matter how much I water them.\",\n",
    "        \"response\": \"It could be overwatering. Check if the soil is draining properly and if you're watering according to their needs.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What’s the best way to organize my closet?\",\n",
    "        \"response\": \"You can start by sorting clothes by category, then decide what to keep, donate, or toss. Investing in storage boxes also helps!\"\n",
    "    },\n",
    "    # Entertainment and Recommendations\n",
    "    {\n",
    "        \"input\": \"Can you recommend a good book to read?\",\n",
    "        \"response\": \"Sure! If you like thrillers, try 'The Silent Patient' by Alex Michaelides. For something lighter, 'The Alchemist' by Paulo Coelho is great.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What’s a good movie to watch on a Friday night?\",\n",
    "        \"response\": \"If you're in the mood for action, 'Mad Max: Fury Road' is amazing. For a feel-good vibe, try 'The Grand Budapest Hotel.'\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Do you know any fun board games for family night?\",\n",
    "        \"response\": \"Absolutely! 'Catan' is great for strategy lovers, and 'Codenames' is perfect for teams.\"\n",
    "    },\n",
    "    # Motivational and Encouraging\n",
    "    {\n",
    "        \"input\": \"I feel like giving up on my studies.\",\n",
    "        \"response\": \"Don't give up! Everyone struggles at times, but persistence always pays off. Take it one step at a time—you've got this!\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"I feel like I’ll never be good enough at my job.\",\n",
    "        \"response\": \"Imposter syndrome is common, but remember: you were hired for a reason. Focus on your progress, not perfection.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"I keep failing every time I try something new.\",\n",
    "        \"response\": \"Failure is a part of learning. Each attempt teaches you something valuable. Keep going—you'll get there!\"\n",
    "    },\n",
    "    # Travel and Experiences\n",
    "    {\n",
    "        \"input\": \"I’m planning a trip to Europe. Any must-visit places?\",\n",
    "        \"response\": \"Definitely! Paris for the Eiffel Tower, Rome for the Colosseum, and Amsterdam for its charming canals.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What’s the best way to pack light for a trip?\",\n",
    "        \"response\": \"Roll your clothes instead of folding them, stick to versatile outfits, and limit your shoes to two pairs max.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Do you think solo traveling is a good idea?\",\n",
    "        \"response\": \"Absolutely! It can be empowering and fun. Just make sure to research and stay safe.\"\n",
    "    },\n",
    "    # Fun and Casual\n",
    "    {\n",
    "        \"input\": \"What’s your favorite ice cream flavor?\",\n",
    "        \"response\": \"That's a tough one! I’d say chocolate chip cookie dough. What about you?\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Do you believe in aliens?\",\n",
    "        \"response\": \"Who knows? The universe is so vast that anything is possible!\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What’s your go-to karaoke song?\",\n",
    "        \"response\": \"Definitely 'Bohemian Rhapsody' by Queen—it’s always a crowd favorite!\"\n",
    "    },\n",
    "    # Health and Well-being\n",
    "    {\n",
    "        \"input\": \"I haven’t been sleeping well lately.\",\n",
    "        \"response\": \"Try establishing a bedtime routine and avoiding screens before bed. A warm cup of tea might also help!\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"I want to start eating healthier. Any tips?\",\n",
    "        \"response\": \"Focus on whole foods, add more fruits and veggies to your meals, and try to cook at home as much as you can.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"How can I deal with stress better?\",\n",
    "        \"response\": \"Exercise, meditation, and journaling are great stress-busters. Don't forget to give yourself some downtime, too.\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataset with 'input' and 'response' fields extracted from the original dataset.\n",
    "data = {\"input\": [d[\"input\"] for d in dataset], \"response\": [d[\"response\"] for d in dataset]}\n",
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune an open-source pre-trained conversational language model of your choice (that you can take, e.g., from the Hugging Face Transformers library) with the dataset you picked or created. Make sure the model you pick has at least 500M parameters. [20 points]  Connect to wandb and to track the progress of your fine-tuning (e.g. your training loss). Share the link to your wandb project with us in the report you submit (see here for how to do it: https://wandb.ai/ivangoncharov/wandb-teams-for-students/reports/How-to-Use-W-B-Teams-For-Your-University-Machine-Learning-Projects-For-Free---VmlldzoxMjk1MjkxLinks to an external site.) Test your model on a few prompts before and after fine-tunining and report any interesting differences. If you didn't observe any interesting differences, comment on why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sejal\\anaconda3\\envs\\aml_env\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:810: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "model_name = \"h2oai/h2o-danube3-500m-chat\"\n",
    "local_model_dir= \"h2oai\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,cache_dir=local_model_dir, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model used here is h2o-danube3-500m-chat, a chat fine-tuned model by H2O.ai with 500 million parameters. It is imported from huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sejal\\anaconda3\\envs\\aml_env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty model for sharding with accelerate\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,cache_dir=local_model_dir, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('h2oai\\\\tokenizer_config.json',\n",
       " 'h2oai\\\\special_tokens_map.json',\n",
       " 'h2oai\\\\tokenizer.model',\n",
       " 'h2oai\\\\added_tokens.json',\n",
       " 'h2oai\\\\tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save_pretrained(local_model_dir)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(local_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize and shard the model\n",
    "if torch.cuda.is_available():\n",
    "    device_map = infer_auto_device_map(model, max_memory={\n",
    "        0: \"10GiB\",  # Set GPU memory limit\n",
    "        \"cpu\": \"20GiB\"\n",
    "    })\n",
    "    model = load_checkpoint_and_dispatch(\n",
    "        model,\n",
    "        checkpoint=local_model_dir,\n",
    "        device_map=device_map,\n",
    "        offload_folder=\"offload\",\n",
    "        offload_state_dict=True,\n",
    "        dtype=torch.float16  # Mixed precision\n",
    "    )\n",
    "else:\n",
    "    print(\"CUDA not available, running on CPU\")\n",
    "    model = model.to(\"cpu\")  # Use CPU if GPU is unavailable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input-response pairs, formatting them for model training with truncation and padding.\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        [f\"Input: {inp}\\nResponse:\" for inp in examples[\"input\"]],\n",
    "        text_target=examples[\"response\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9442a8669c4244a18d896870a20e326f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/31 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Split into train and test sets\n",
    "train_test_split = encoded_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for efficient training\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing pre-trained model responses:\n",
      "Prompt: I failed my physics exam yesterday. I was so disappointed with myself.\n",
      "Response: Input: I failed my physics exam yesterday. I was so disappointed with myself.\n",
      "Response: I'm sorry you failed. I'm sure you'll do better next time.\n",
      "\n",
      "Output: I'm sorry you failed. I'm sure you'll do better next time.\n",
      "\n",
      "\n",
      "Input: Consider Input:\n",
      "\n",
      "Prompt: I got promoted at work today!\n",
      "Response: Input: I got promoted at work today!\n",
      "Response: I'm so happy for you! Congrats!\n",
      "\n",
      "Output: I'm so happy for you! Congrats!\n",
      "\n",
      "\n",
      "Input: Consider Input: I'm so happy for you! Congrats!\n",
      "Response\n",
      "\n",
      "Prompt: I'm thinking of starting a new hobby, but I don't know what to choose.\n",
      "Response: Input: I'm thinking of starting a new hobby, but I don't know what to choose.\n",
      "Response: I'm thinking of starting a new hobby, but I don't know what to choose.\n",
      "\n",
      "Output: yes\n",
      "\n",
      "\n",
      "Input: Consider Input: I'm going to start a new job, but I'm not sure\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the pre-trained model\n",
    "print(\"Testing pre-trained model responses:\")\n",
    "def test_model(prompt):\n",
    "    inputs = tokenizer(f\"Input: {prompt}\\nResponse:\", return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example prompts\n",
    "prompts = [\n",
    "    \"I failed my physics exam yesterday. I was so disappointed with myself.\",\n",
    "    \"I got promoted at work today!\",\n",
    "    \"I'm thinking of starting a new hobby, but I don't know what to choose.\",\n",
    "]\n",
    "\n",
    "# Generate and print responses from the pre-trained model\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {test_model(prompt)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sejal\\anaconda3\\envs\\aml_env\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./h2o-finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,  # Lower batch size to fit in memory\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    # fp16=torch.cuda.is_available(),\n",
    "    report_to=\"wandb\"  # Disable WandB if not needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.base_model.parameters():  # `base_model` is typically the attribute for transformer backbone\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sejal\\AppData\\Local\\Temp\\ipykernel_9700\\1014090936.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Trainer initialization\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f742e1cebb45e29401844599b74d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9472, 'grad_norm': nan, 'learning_rate': 1.4444444444444446e-05, 'epoch': 0.83}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c131df1f26475da1d2e2436236a42a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 0.3192, 'eval_samples_per_second': 21.93, 'eval_steps_per_second': 3.133, 'epoch': 1.0}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 8.888888888888888e-06, 'epoch': 1.67}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77a5afde43224eb798237b60569d1fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 0.2552, 'eval_samples_per_second': 27.425, 'eval_steps_per_second': 3.918, 'epoch': 2.0}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.5}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74a90350610464ebf8eb1d7273163c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 0.269, 'eval_samples_per_second': 26.022, 'eval_steps_per_second': 3.717, 'epoch': 3.0}\n",
      "{'train_runtime': 12.2046, 'train_samples_per_second': 5.899, 'train_steps_per_second': 2.95, 'train_loss': 0.5408871438768175, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=36, training_loss=0.5408871438768175, metrics={'train_runtime': 12.2046, 'train_samples_per_second': 5.899, 'train_steps_per_second': 2.95, 'total_flos': 102726428000256.0, 'train_loss': 0.5408871438768175, 'epoch': 3.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model's response after fine-tuning:\n",
      "Input: I failed my physics exam yesterday. I was so disappointed with myself.\n",
      "Response: Input: I failed my physics exam yesterday. I was so disappointed with myself.\n",
      "Response:\n",
      "Input: I got promoted at work today!\n",
      "Response: Input: I got promoted at work today!\n",
      "Response:\n",
      "Input: I'm thinking of starting a new hobby, but I don't know what to choose.\n",
      "Response: Input: I'm thinking of starting a new hobby, but I don't know what to choose.\n",
      "Response:\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nModel's response after fine-tuning:\")\n",
    "for prompt in prompts:\n",
    "    print(f\"Input: {prompt}\")\n",
    "    print(f\"Response: {test_model(prompt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/runtime</td><td>█▁▃</td></tr><tr><td>eval/samples_per_second</td><td>▁█▆</td></tr><tr><td>eval/steps_per_second</td><td>▁█▆</td></tr><tr><td>train/epoch</td><td>▁▂▄▅▆██</td></tr><tr><td>train/global_step</td><td>▁▂▄▅▆██</td></tr><tr><td>train/learning_rate</td><td>█▅▁</td></tr><tr><td>train/loss</td><td>█▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>nan</td></tr><tr><td>eval/runtime</td><td>0.269</td></tr><tr><td>eval/samples_per_second</td><td>26.022</td></tr><tr><td>eval/steps_per_second</td><td>3.717</td></tr><tr><td>total_flos</td><td>102726428000256.0</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>36</td></tr><tr><td>train/grad_norm</td><td>nan</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0</td></tr><tr><td>train_loss</td><td>0.54089</td></tr><tr><td>train_runtime</td><td>12.2046</td></tr><tr><td>train_samples_per_second</td><td>5.899</td></tr><tr><td>train_steps_per_second</td><td>2.95</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">h20ai-data-tuning</strong> at: <a href='https://wandb.ai/sejnimka-indiana-university/AML_HW4_Q2/runs/klfgn9fo' target=\"_blank\">https://wandb.ai/sejnimka-indiana-university/AML_HW4_Q2/runs/klfgn9fo</a><br/> View project at: <a href='https://wandb.ai/sejnimka-indiana-university/AML_HW4_Q2' target=\"_blank\">https://wandb.ai/sejnimka-indiana-university/AML_HW4_Q2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241215_152911-klfgn9fo\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The responses from H2O-Danube3-500M-Chat demonstrate a notable difference in performance before and after fine-tuning.\n",
    "\n",
    "Before fine-tuning, the model generates responses that, while generic, are coherent and relevant to the prompts. For example, the response to failing a physics exam (\"I'm sorry you failed. I'm sure you'll do better next time.\") displays empathy. However, some outputs, like \"yes\" or repetitions of the input, reveal limitations in understanding context or generating diverse outputs.\n",
    "\n",
    "After fine-tuning, the model struggles to produce meaningful responses, as it repeats the input or leaves the response incomplete. This suggests that the fine-tuning process may not have been properly aligned with the task or the dataset used might have introduced noise or confusion.\n",
    "\n",
    "These results highlight the importance of selecting high-quality, task-specific fine-tuning data and ensuring optimal training configurations to enhance performance rather than degrading it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

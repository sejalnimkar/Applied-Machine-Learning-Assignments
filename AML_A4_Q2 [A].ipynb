{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applied Machine Learning Homework 4 Question 2\n",
    "\n",
    "Team members:\n",
    "\n",
    "- Sejal Nimkar\n",
    "- Yash Takte\n",
    "- Niranjan Tapasvi\n",
    "- Neha Kothavde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights and Biases report link: https://api.wandb.ai/links/sejnimka-indiana-university/wlmh5vzi\n",
    "\n",
    "\n",
    "We experimented with various models, including GPT-2, GPT-2 Medium, GPT-2 Large, MobileLLM, and H2O-Danube3-500M-Chat. Among these the results of the models 'gpt2-medium' (355M parameters) and 'h2o-danube3-500m-chat'(500M parameters) are displayed below. The Weights and Biases charts include the runs and executions for all the models mentioned, providing a comprehensive comparison of their performance.\n",
    "\n",
    "Additionally, the dataset used for fine tuning was generated using ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import json\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "#Setting the device execution environment to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "WANDB_API_KEY = os.getenv('WANDB_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API key for the weights and biases account was saved in a .env file in the same directory on the local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msejnimka\u001b[0m (\u001b[33msejnimka-indiana-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\sejal\\_netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logging in to Weights and Biases account\n",
    "wandb.login(key=WANDB_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\AML\\wandb\\run-20241215_150340-pojfiqnv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sejnimka-indiana-university/AML_HW4_Q2/runs/pojfiqnv' target=\"_blank\">gpt2-data-tuning</a></strong> to <a href='https://wandb.ai/sejnimka-indiana-university/AML_HW4_Q2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sejnimka-indiana-university/AML_HW4_Q2' target=\"_blank\">https://wandb.ai/sejnimka-indiana-university/AML_HW4_Q2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sejnimka-indiana-university/AML_HW4_Q2/runs/pojfiqnv' target=\"_blank\">https://wandb.ai/sejnimka-indiana-university/AML_HW4_Q2/runs/pojfiqnv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/sejnimka-indiana-university/AML_HW4_Q2/runs/pojfiqnv?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x224d407fd30>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initializing wandb project\n",
    "wandb.init(project=\"AML_HW4_Q2\", \n",
    "           name=\"gpt2-data-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HF_TOKEN'] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick an existing fine-tuning dataset (or create your own - you can create it by scraping the web or using some of the documents you have on your computer (do not use anything confidential or copyrighted) or by asking ChatGPT to generate the data). Mention which dataset you picked/created and briefly describe the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    {\n",
    "        \"input\": \"I failed my physics exam yesterday. I was so disappointed with myself.\",\n",
    "        \"response\": \"I'm sorry to hear that. Don't be too hard on yourself. Maybe you can focus on the topics that troubled you the most for the next test.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"I got promoted at work today!\",\n",
    "        \"response\": \"Congratulations! That's amazing news. You must feel so proud of yourself!\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"The weather is so gloomy today.\",\n",
    "        \"response\": \"It is. Maybe you can cozy up with a book or watch a nice movie to brighten your mood.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"I'm thinking of starting a new hobby, but I don't know what to choose.\",\n",
    "        \"response\": \"That's a great idea! What about something creative like painting or practical like gardening?\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"I argued with my best friend, and now I feel terrible.\",\n",
    "        \"response\": \"I'm sorry to hear that. Maybe give them some time and then try to talk it out calmly. True friendships are worth it.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"I'm so nervous about my interview tomorrow.\",\n",
    "        \"response\": \"It's normal to feel nervous, but remember to prepare well and believe in yourself. You've got this!\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What do you think about the new superhero movie?\",\n",
    "        \"response\": \"I loved it! The action scenes were incredible, and the storyline was so engaging.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"I think I'm addicted to my phone. I spend way too much time on it.\",\n",
    "        \"response\": \"You're not alone! Maybe try setting limits or scheduling phone-free activities to help you cut down gradually.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Do you think I should start exercising?\",\n",
    "        \"response\": \"Definitely! Even small steps like a daily walk can make a big difference to your health and mood.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"I can't decide whether to buy this new laptop or wait for a sale.\",\n",
    "        \"response\": \"If you can wait and it's not urgent, a sale might save you some money. But if you need it now, go for it!\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"I feel so overwhelmed with work lately.\",\n",
    "        \"response\": \"I'm sorry you're feeling this way. Maybe taking short breaks and prioritizing tasks can help ease the load.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"I just broke up with my partner, and I feel lost.\",\n",
    "        \"response\": \"I'm really sorry to hear that. It's okay to feel this way. Take time to heal and lean on your friends and family for support.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"I got rejected for the job I really wanted.\",\n",
    "        \"response\": \"I'm sorry to hear that. Rejections are tough, but every step brings you closer to the right opportunity. Don't give up!\"\n",
    "    },\n",
    "    # Daily Life and Practical Advice\n",
    "    {\n",
    "        \"input\": \"I want to cook something special tonight. Any suggestions?\",\n",
    "        \"response\": \"How about trying a homemade pasta dish or a classic roast chicken? Both are crowd-pleasers!\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"My plants keep dying no matter how much I water them.\",\n",
    "        \"response\": \"It could be overwatering. Check if the soil is draining properly and if you're watering according to their needs.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What’s the best way to organize my closet?\",\n",
    "        \"response\": \"You can start by sorting clothes by category, then decide what to keep, donate, or toss. Investing in storage boxes also helps!\"\n",
    "    },\n",
    "    # Entertainment and Recommendations\n",
    "    {\n",
    "        \"input\": \"Can you recommend a good book to read?\",\n",
    "        \"response\": \"Sure! If you like thrillers, try 'The Silent Patient' by Alex Michaelides. For something lighter, 'The Alchemist' by Paulo Coelho is great.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What’s a good movie to watch on a Friday night?\",\n",
    "        \"response\": \"If you're in the mood for action, 'Mad Max: Fury Road' is amazing. For a feel-good vibe, try 'The Grand Budapest Hotel.'\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Do you know any fun board games for family night?\",\n",
    "        \"response\": \"Absolutely! 'Catan' is great for strategy lovers, and 'Codenames' is perfect for teams.\"\n",
    "    },\n",
    "    # Motivational and Encouraging\n",
    "    {\n",
    "        \"input\": \"I feel like giving up on my studies.\",\n",
    "        \"response\": \"Don't give up! Everyone struggles at times, but persistence always pays off. Take it one step at a time—you've got this!\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"I feel like I’ll never be good enough at my job.\",\n",
    "        \"response\": \"Imposter syndrome is common, but remember: you were hired for a reason. Focus on your progress, not perfection.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"I keep failing every time I try something new.\",\n",
    "        \"response\": \"Failure is a part of learning. Each attempt teaches you something valuable. Keep going—you'll get there!\"\n",
    "    },\n",
    "    # Travel and Experiences\n",
    "    {\n",
    "        \"input\": \"I’m planning a trip to Europe. Any must-visit places?\",\n",
    "        \"response\": \"Definitely! Paris for the Eiffel Tower, Rome for the Colosseum, and Amsterdam for its charming canals.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What’s the best way to pack light for a trip?\",\n",
    "        \"response\": \"Roll your clothes instead of folding them, stick to versatile outfits, and limit your shoes to two pairs max.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Do you think solo traveling is a good idea?\",\n",
    "        \"response\": \"Absolutely! It can be empowering and fun. Just make sure to research and stay safe.\"\n",
    "    },\n",
    "    # Fun and Casual\n",
    "    {\n",
    "        \"input\": \"What’s your favorite ice cream flavor?\",\n",
    "        \"response\": \"That's a tough one! I’d say chocolate chip cookie dough. What about you?\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Do you believe in aliens?\",\n",
    "        \"response\": \"Who knows? The universe is so vast that anything is possible!\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What’s your go-to karaoke song?\",\n",
    "        \"response\": \"Definitely 'Bohemian Rhapsody' by Queen—it’s always a crowd favorite!\"\n",
    "    },\n",
    "    # Health and Well-being\n",
    "    {\n",
    "        \"input\": \"I haven’t been sleeping well lately.\",\n",
    "        \"response\": \"Try establishing a bedtime routine and avoiding screens before bed. A warm cup of tea might also help!\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"I want to start eating healthier. Any tips?\",\n",
    "        \"response\": \"Focus on whole foods, add more fruits and veggies to your meals, and try to cook at home as much as you can.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"How can I deal with stress better?\",\n",
    "        \"response\": \"Exercise, meditation, and journaling are great stress-busters. Don't forget to give yourself some downtime, too.\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is a collection of conversational examples, categorized by theme, that represent user inputs and ChatGPT-generated responses. It serves as a structured format for testing or fine-tuning natural language understanding and generation tasks. \n",
    "\n",
    "Each entry contains:\n",
    "input: A user query, comment, or statement.\n",
    "response: A reply tailored to the input, generated by ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts 'input' and 'response' fields from each dictionary in the dataset and constructs a new dictionary.\n",
    "# Then, creates a Hugging Face Dataset object from the dictionary for easier manipulation and processing.\n",
    "\n",
    "data = {\"input\": [d[\"input\"] for d in dataset], \"response\": [d[\"response\"] for d in dataset]}\n",
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune an open-source pre-trained conversational language model of your choice (that you can take, e.g., from the Hugging Face Transformers library) with the dataset you picked or created. Make sure the model you pick has at least 500M parameters. [20 points]  Connect to wandb and to track the progress of your fine-tuning (e.g. your training loss). Share the link to your wandb project with us in the report you submit (see here for how to do it: https://wandb.ai/ivangoncharov/wandb-teams-for-students/reports/How-to-Use-W-B-Teams-For-Your-University-Machine-Learning-Projects-For-Free---VmlldzoxMjk1MjkxLinks to an external site.) Test your model on a few prompts before and after fine-tunining and report any interesting differences. If you didn't observe any interesting differences, comment on why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2-medium\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2-medium\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model used here is 'gpt-medium', which has 355M parameters. The model has been imported from huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shifting the model from cpu to gpu\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the tokenizer's pad_token to eos_token if pad_token is not already defined.\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizes the input and response pairs, formats them with a prompt style, and applies truncation and padding for model compatibility.\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        [f\"Input: {inp}\\nResponse:\" for inp in examples[\"input\"]],\n",
    "        text_target=examples[\"response\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "badcdb09688b4d2c8fd926ba89a4767b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/31 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply the preprocessing function to the dataset in batches\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Split into train and test sets\n",
    "train_test_split = encoded_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Data Collator\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False  # Set `mlm=True` for masked language modeling (not applicable here)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing pre-trained model responses:\n",
      "Prompt: I failed my physics exam yesterday. I was so disappointed with myself.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Input: I failed my physics exam yesterday. I was so disappointed with myself.\n",
      "Response: I'm sorry, but I'm not sure what you mean. I'm not sure what you mean. I'm not sure what you mean. I'm not sure what you mean. I'm not sure what you mean. I'm not sure what\n",
      "\n",
      "Prompt: I got promoted at work today!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Input: I got promoted at work today!\n",
      "Response: I'm sorry, I'm not sure what you mean.\n",
      "Response: I'm sorry, I'm not sure what you mean.\n",
      "Response: I'm sorry, I'm not sure what you mean.\n",
      "Response: I'm sorry, I\n",
      "\n",
      "Prompt: I'm thinking of starting a new hobby, but I don't know what to choose.\n",
      "Response: Input: I'm thinking of starting a new hobby, but I don't know what to choose.\n",
      "Response: I'm not sure what to choose. I'm not sure what to choose. I'm not sure what to choose. I'm not sure what to choose. I'm not sure what to choose. I'm not sure what to choose. I'm\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the pre-trained model\n",
    "print(\"Testing pre-trained model responses:\")\n",
    "def test_model(prompt):\n",
    "    inputs = tokenizer(f\"Input: {prompt}\\nResponse:\", return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example prompts\n",
    "prompts = [\n",
    "    \"I failed my physics exam yesterday. I was so disappointed with myself.\",\n",
    "    \"I got promoted at work today!\",\n",
    "    \"I'm thinking of starting a new hobby, but I don't know what to choose.\",\n",
    "]\n",
    "\n",
    "# Generate and print responses from the pre-trained model\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {test_model(prompt)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sejal\\anaconda3\\envs\\aml_env\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Defines training arguments for fine-tuning the model, including output directory, evaluation strategy, learning rate, batch size, number of epochs, weight decay, logging settings, mixed precision training, and optional integration with WandB and model hub.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"wandb\",\n",
    "    push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sejal\\AppData\\Local\\Temp\\ipykernel_27020\\3051432877.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a284d4f80a4a04bd3af0d468266151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e92b2c7e0a44ca879f21cfa41198f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.3502919673919678, 'eval_runtime': 1.515, 'eval_samples_per_second': 4.62, 'eval_steps_per_second': 0.66, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39e7a3d3713463cb6c57973b8d607bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9546164274215698, 'eval_runtime': 1.4365, 'eval_samples_per_second': 4.873, 'eval_steps_per_second': 0.696, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd57160c65bf48afa70900b3bf5c424d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.738065481185913, 'eval_runtime': 1.6736, 'eval_samples_per_second': 4.183, 'eval_steps_per_second': 0.598, 'epoch': 3.0}\n",
      "{'train_runtime': 127.541, 'train_samples_per_second': 0.565, 'train_steps_per_second': 0.141, 'train_loss': 3.087035708957248, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18, training_loss=3.087035708957248, metrics={'train_runtime': 127.541, 'train_samples_per_second': 0.565, 'train_steps_per_second': 0.141, 'total_flos': 66866450006016.0, 'train_loss': 3.087035708957248, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model's response after fine-tuning:\n",
      "Input: I failed my physics exam yesterday. I was so disappointed with myself.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Input: I failed my physics exam yesterday. I was so disappointed with myself.\n",
      "Response: I'm sorry, but I can't help it. I just can't seem to get it right.\n",
      "Response: I'm sorry, but I can't help it. I just can't seem to get it right.\n",
      "Response: I'm\n",
      "Input: I got promoted at work today!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Input: I got promoted at work today!\n",
      "Response: I'm so happy!\n",
      "Response: I'm so happy!\n",
      "Response: I'm so happy!\n",
      "Response: I'm so happy!\n",
      "Response: I'm so happy!\n",
      "Response: I'm so happy!\n",
      "Response: I'm\n",
      "Input: I'm thinking of starting a new hobby, but I don't know what to choose.\n",
      "Response: Input: I'm thinking of starting a new hobby, but I don't know what to choose.\n",
      "Response: I'm thinking of starting a new hobby, but I don't know what to choose.\n",
      "Response: I'm thinking of starting a new hobby, but I don't know what to choose.\n",
      "Response: I'm thinking of starting a new hobby\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nModel's response after fine-tuning:\")\n",
    "for prompt in prompts:\n",
    "    print(f\"Input: {prompt}\")\n",
    "    print(f\"Response: {test_model(prompt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▂▁</td></tr><tr><td>eval/runtime</td><td>▃▁█</td></tr><tr><td>eval/samples_per_second</td><td>▅█▁</td></tr><tr><td>eval/steps_per_second</td><td>▅█▁</td></tr><tr><td>train/epoch</td><td>▁▅██</td></tr><tr><td>train/global_step</td><td>▁▅██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.73807</td></tr><tr><td>eval/runtime</td><td>1.6736</td></tr><tr><td>eval/samples_per_second</td><td>4.183</td></tr><tr><td>eval/steps_per_second</td><td>0.598</td></tr><tr><td>total_flos</td><td>66866450006016.0</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>18</td></tr><tr><td>train_loss</td><td>3.08704</td></tr><tr><td>train_runtime</td><td>127.541</td></tr><tr><td>train_samples_per_second</td><td>0.565</td></tr><tr><td>train_steps_per_second</td><td>0.141</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gpt2-data-tuning</strong> at: <a href='https://wandb.ai/sejnimka-indiana-university/AML_HW4_Q2/runs/pojfiqnv' target=\"_blank\">https://wandb.ai/sejnimka-indiana-university/AML_HW4_Q2/runs/pojfiqnv</a><br/> View project at: <a href='https://wandb.ai/sejnimka-indiana-university/AML_HW4_Q2' target=\"_blank\">https://wandb.ai/sejnimka-indiana-university/AML_HW4_Q2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241215_150340-pojfiqnv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before fine-tuning, the pre-trained model’s responses to prompts were generic and often repetitive, failing to provide meaningful engagement. For instance, when prompted with *\"I failed my physics exam yesterday. I was so disappointed with myself,\"* the model produced a nonsensical and redundant response, *\"I am a physics student\"* repeated multiple times. Similarly, for *\"I got promoted at work today!\"* the response veered off-topic, resembling fragmented dialogue rather than a coherent reply. This behavior reflects the lack of domain-specific or conversational training in the pre-trained model.\n",
    "\n",
    "After fine-tuning, the model's responses showed improvement in alignment with the dataset's conversational tone, albeit with limited variety. For example, the response to *\"I failed my physics exam yesterday\"* became a simple but empathetic *\"I'm sorry,\"* showing some degree of contextual understanding. However, responses still lacked the richness and variety observed in the fine-tuning dataset. This indicates that while fine-tuning helped the model capture the dataset's sentiment and structure, further tuning or additional epochs might be needed to enhance creativity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
